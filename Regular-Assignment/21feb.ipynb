{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8961e1c-2223-4a99-82f7-01111721297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
    "\n",
    "Web scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML \n",
    "format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications. \n",
    "There are many different ways to perform web scraping to obtain data from websites. These include using online services,\n",
    "particular API’s or even creating your code for web scraping from scratch. Many large websites, like Google, Twitter, \n",
    "Facebook, StackOverflow, etc. have API’s that allow you to access their data in a structured format. \n",
    "This is the best option, but there are other sites that don’t allow users to access large amounts of data in a \n",
    "structured form or they are simply not that technologically advanced. In that situation, it’s best to use Web Scraping to \n",
    "scrape the website for data.\n",
    "\n",
    "Web scraping requires two parts, namely the crawler and the scraper. The crawler is an artificial intelligence algorithm that browses \n",
    "the web to search for the particular data required by following the links across the internet. The scraper, on the other hand, is \n",
    "a specific tool created to extract data from the website. The design of the scraper can vary greatly according to the complexity and \n",
    "scope of the project so that it can quickly and accurately extract the data.\n",
    "\n",
    "Some of the main use cases of web scraping include\n",
    "\n",
    "Price monitoring\n",
    "Price intelligence\n",
    "News monitoring\n",
    "Lead generation\n",
    "market research\n",
    "Sentiment Analysis\n",
    "Email Marketing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q2. What are the different methods used for Web Scraping?\n",
    "\n",
    "Different Types of Web Scrapers Web Scrapers can be divided on the basis of many different criteria, including Self-built or Pre-built\n",
    "Web Scrapers, Browser extension or Software Web Scrapers, and Cloud or Local Web Scrapers.\n",
    "You can have Self-built Web Scrapers but that requires advanced knowledge of programming. And if you want more features in your \n",
    "Web Scraper, then you need even more knowledge. On the other hand, pre-built Web Scrapers are previously created scrapers that you \n",
    "can download and run easily. These also have more advanced options that you can customize.\n",
    "\n",
    "Browser extensions Web Scrapers are extensions that can be added to your browser. These are easy to run as they are integrated with \n",
    "your browser, but at the same time, they are also limited because of this. Any advanced features that are outside the scope of your\n",
    "browser are impossible to run on Browser extension Web Scrapers. But Software Web Scrapers don’t have these limitations as they can be \n",
    "downloaded and installed on your computer. These are more complex than Browser web scrapers, but they also have advanced features that\n",
    "are not limited by the scope of your browser.\n",
    "\n",
    "Cloud Web Scrapers run on the cloud, which is an off-site server mostly provided by the company that you buy the scraper from. \n",
    "These allow your computer to focus on other tasks as the computer resources are not required to scrape data from websites. Local\n",
    "Web Scrapers, on the other hand, run on your computer using local resources. So, if the Web scrapers require more CPU or RAM, then \n",
    "your computer will become slow and not be able to perform other tasks.\n",
    "\n",
    "Here are a few techniques commonly used to scrape data from websites. In general, all web scraping techniques retrieve content \n",
    "from websites, process it using a scraping engine, and generate one or more data files with the extracted content.\n",
    "\n",
    "HTML Parsing\n",
    "HTML parsing involves the use of JavaScript to target a linear or nested HTML page. It is a powerful and fast method for extracting \n",
    "text and links (e.g. a nested link or email address), scraping screens and pulling resources.\n",
    "\n",
    "DOM Parsing\n",
    "The Document Object Model (DOM) defines the structure, style and content of an XML file. Scrapers typically use a DOM parser to view \n",
    "the structure of web pages in depth. DOM parsers can be used to access the nodes that contain information and scrape the web page with\n",
    "tools like XPath. For dynamically generated content, scrapers can embed web browsers like Firefox and Internet Explorer to extract \n",
    "whole web pages (or parts of them).\n",
    "\n",
    "Vertical Aggregation\n",
    "Companies that use extensive computing power can create vertical aggregation platforms to target particular verticals.\n",
    "These are data harvesting platforms that can be run on the cloud and are used to automatically generate and monitor bots for \n",
    "certain verticals with minimal human intervention. Bots are generated according to the information required to each vertical, \n",
    "and their efficiency is determined by the quality of data they extract.\n",
    "\n",
    "XPath\n",
    "XPath is short for XML Path Language, which is a query language for XML documents. XML documents have tree-like structures,\n",
    "so scrapers can use XPath to navigate through them by selecting nodes according to various parameters. A scraper may combine\n",
    "DOM parsing with XPath to extract whole web pages and publish them on a destination site.\n",
    "\n",
    "Google Sheets\n",
    "Google Sheets is a popular tool for data scraping. Scarpers can use the IMPORTXML function in Sheets to scrape from a website,\n",
    "which is useful if they want to extract a specific pattern or data from the website. This command also makes it possible to check \n",
    "if a website can be scraped or is protected.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q3. What is Beautiful Soup? Why is it used?\n",
    "\n",
    "Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic\n",
    "ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work.It was first\n",
    "introduced by Leonard Richardson, who is still contributing to this project and this project is additionally supported by Tidelift \n",
    "(a paid subscription tool for open-source maintenance)\n",
    "\n",
    "Beautiful soup3 was officially released in May 2006, Latest version released by Beautiful Soup is 4.9.2, and it supports \n",
    "Python 3 and Python 2.4 as well.\n",
    "\n",
    "Beautiful Soup provides simple methods for navigating, searching, and modifying a parse tree in HTML, XML files. \n",
    "It transforms a complex HTML document into a tree of Python objects. It also automatically converts the document to Unicode, \n",
    "so you don't have to think about encodings.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q4. Why is flask used in this Web Scraping project?\n",
    "\n",
    "Flask is a fast ,lightweight and Beginners friendly web Framework written in Python which uses Werkzeug WSGI toolkit and \n",
    "Jinja2 template engine.It is an easy framework to build websites. We can use this to parse our collected data and display it as HTML\n",
    "in a new HTML file. The requests module allows us to send http requests to the website we want to scrape. The first line imports the\n",
    "Flask class and the render_template method from the flask library. Flask can also be used to dump the scrapped data in MongoDb or \n",
    "can be deployed in real time using AWS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n",
    "\n",
    "CODEPIPELINE\n",
    "\n",
    "AWS CodePipeline console or the AWS CLI can be used to create a pipeline. Pipelines must have at least two stages. The first stage\n",
    "of a pipeline must be a source stage. The pipeline must have at least one other stage that is a build or deployment stage.\n",
    "\n",
    "You can add actions to your pipeline that are in an AWS Region different from your pipeline. A cross-Region action is one in which \n",
    "an AWS service is the provider for an action and the action type or provider type are in an AWS Region different from your pipeline. \n",
    "For more information, see Add a cross-Region action in CodePipeline.\n",
    "\n",
    "You can also create pipelines that build and deploy container-based applications by using Amazon ECS as the deployment provider. \n",
    "Before you create a pipeline that deploys container-based applications with Amazon ECS, you must create an image definitions file as\n",
    "described in Image definitions file reference.\n",
    "\n",
    "CodePipeline uses change detection methods to start your pipeline when a source code change is pushed. These detection methods \n",
    "are based on source type:\n",
    "\n",
    "CodePipeline uses Amazon CloudWatch Events to detect changes in your CodeCommit source repository and branch or your S3 source bucket.\n",
    "\n",
    "ELASTIC BEANSTALK\n",
    "\n",
    "With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure\n",
    "that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. \n",
    "You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load \n",
    "balancing, scaling, and application health monitoring.\n",
    "\n",
    "Elastic Beanstalk supports applications developed in Go, Java, .NET, Node.js, PHP, Python, and Ruby. When you deploy your\n",
    "application, Elastic Beanstalk builds the selected supported platform version and provisions one or more AWS resources, such as\n",
    "Amazon EC2 instances, to run your application.\n",
    "\n",
    "You can interact with Elastic Beanstalk by using the Elastic Beanstalk console, the AWS Command Line Interface (AWS CLI), \n",
    "or eb, a high-level CLI designed specifically for Elastic Beanstalk.\n",
    "\n",
    "To learn more about how to deploy a sample web application using Elastic Beanstalk, see Getting Started with AWS: \n",
    "    Deploying a Web App.\n",
    "\n",
    "You can also perform most deployment tasks, such as changing the size of your fleet of Amazon EC2 instances or monitoring \n",
    "your application, directly from the Elastic Beanstalk web interface (console)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
